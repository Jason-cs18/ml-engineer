===========
AIGC System
===========

Memory managment for LLM serving
--------------------------------
LLM inference demands a lot of memory for KV caches. Especially for long-context requests and high concurrent requests, the large memory usage limits the scalability of LLM serving.

To improve this, three techniques are widely used: continuous batching, paged attention, and vAttention.

Orca

LLM based agents development
--------------------------------
What are LLM based agents?

How to build LLM based agents?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
workflow and tools

How to optimize LLm based agents?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
challenges and solutions

Best practices
^^^^^^^^^^^^^^
tips for building and optimizing compound AI systems

Case studies on researchers
^^^^^^^^^^^^^^^^^^^^^^^^^^^
an autonmous AI agent can draft a research proposal and write a paper.

Case studies on expert-level assistants
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
an AI agent can assist a people to analysis data and write a report.